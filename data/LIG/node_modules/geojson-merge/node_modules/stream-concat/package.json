{
  "name": "stream-concat",
  "version": "0.1.0",
  "description": "Simple and efficient node stream concatenation.",
  "main": "index.js",
  "repository": {
    "type": "git",
    "url": "https://github.com/sedenardi/node-stream-concat.git"
  },
  "keywords": [
    "node",
    "stream",
    "concat",
    "streams",
    "concatenation"
  ],
  "author": {
    "name": "Sanders DeNardi",
    "email": "sedenardi@gmail.com",
    "url": "http://www.sandersdenardi.com/"
  },
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/sedenardi/node-stream-concat/issues"
  },
  "homepage": "https://github.com/sedenardi/node-stream-concat",
  "readme": "# node-stream-concat\nSimple and efficient node stream concatenation.\n\n`node-stream-concat` concatenates several streams into one single readable stream. The input streams can either be existing streams or can be determined on the fly by a user specified function.\n\n    npm install stream-concat\n\n# Usage\n\n    var StreamConcat = require('stream-concat');\n    var combinedStream = new StreamConcat(streams,[options]);\n\n## streams\nThe simplest way to use StreamConcat is to supply an array of readable streams.\n\n    var fs = require('fs');\n\n    var stream1 = createReadStream('file1.csv');\n    var stream2 = createReadStream('file2.csv');\n    var stream3 = createReadStream('file3.csv');\n\n    var combinedStream = new StreamConcat([stream1, stream2, stream3]);\n\nHowever, when working with large amounts of data, this can lead to high memory usage and relatively poor performance (versus the original stream). This is because all streams' read queues are buffered and waiting to be read.\n\nA better way is to defer opening a new stream until the moment it's needed. You can do this by passing a function into the constructor that returns the next available stream, or `null` if there are no more streams.\n\nIf we're reading from several large files, we can do the following.\n\n    var fs = require('fs');\n\n    var fileNames = ['file1.csv', 'file2.csv', 'file3.csv'];\n    var fileIndex = 0;\n    var nextStream = function() {\n      if (fileIndex === fileNames.length) \n        return null;\n\n      return fs.createReadStream(fileNames[fileIndex++]);\n    };\n\n    var combinedStream = new StreamConcat(nextStream);\n\nOnce StreamConcat is done with a stream it'll call `nextStream` and start using the returned stream (if not null);\n\n## options\nThese are standard `Stream` [options](http://nodejs.org/api/stream.html#stream_new_stream_transform_options) passed to the underlying `Transform` stream.\n\n* `highWaterMark` Number The maximum number of bytes to store in the internal buffer before ceasing to read from the underlying resource. Default=16kb\n* `encoding` String If specified, then buffers will be decoded to strings using the specified encoding. Default=null\n* `objectMode` Boolean Whether this stream should behave as a stream of objects. Meaning that stream.read(n) returns a single value instead of a Buffer of size n. Default=false\n\n## StreamConcat.addStream(newStream)\nIf you've created the StreamConcat object from an array of streams, you can use `addStream()` as long as the last stream hasn't finishing being read (StreamConcat hasn't emitted the `end` event).\n\nTo add streams to a StreamConcat object created from a function, you should modify the underlying data that the function is accessing.\n",
  "readmeFilename": "README.md",
  "_id": "stream-concat@0.1.0",
  "dist": {
    "shasum": "fa98d993b5bf9cb675f140ee56b2628c8d7fd8b7"
  },
  "_from": "stream-concat@0.1.0",
  "_resolved": "https://registry.npmjs.org/stream-concat/-/stream-concat-0.1.0.tgz"
}
